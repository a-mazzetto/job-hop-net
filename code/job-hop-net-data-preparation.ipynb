{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d3404a",
   "metadata": {},
   "source": [
    "# Job Hop Net Data Preparation\n",
    "## A. Mazzetto\n",
    "### December 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a7e093",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, timedelta\n",
    "from dateparser import parse\n",
    "import itertools\n",
    "import re\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4f9f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4808be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from deep_translator import GoogleTranslator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d68853",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "Please skip to the Data Analysis section if not interested in the data preparation. This might take some time due to the Google Translation and Google NGram query. Note that this is currently based on a tiny part of the dataset, jsut for exemplification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5cabde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "rawdata0 = pd.read_csv('..\\data\\dataset-tiny-sample.csv', index_col= 0)\n",
    "\n",
    "# Names are often present only for the first work esperience\n",
    "rawdata0['Name'] = rawdata0['Name'].fillna(method='ffill')\n",
    "\n",
    "# Drop rows with NA\n",
    "rawdata0 = rawdata0.dropna()\n",
    "rawdata0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b4b345",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# These are commented as they were useful only on the original, non-anonymous dataset\n",
    "\n",
    "# Drop columnd Serial No. as not reliable\n",
    "rawdata0 = rawdata0.drop(columns= ['1'])\n",
    "# Anonymize the dataset\n",
    "rawdata0['Name'] = rawdata0['Name'].apply(lambda x: hashlib.sha1(x.encode(\"utf-8\")).hexdigest())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e30b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4660467e",
   "metadata": {},
   "source": [
    "# Date Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cb8868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse dates: substitute string 'Present' with current date\n",
    "rawdata1 = rawdata0.copy()\n",
    "rawdata1['To'][rawdata1['To'].str.lower().str.find('present')>-1] = date.today().strftime(\"%Y-%m\") # YM\n",
    "rawdata1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77965bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse dates: transform two-digit numbers into year if there is a string for the month\n",
    "current_year_int = int(str(date.today().year)[-2:])\n",
    "def expand_year(match):\n",
    "    token = match.group(2)\n",
    "    token_new = ''\n",
    "    if int(token) <= current_year_int:\n",
    "        if len(match.group(2)) == 1:\n",
    "            token_new = '200' + token\n",
    "        else:\n",
    "            token_new = '20' + token\n",
    "    else:\n",
    "        token_new = '19' + token\n",
    "    return match.group(1) + token_new + match.group(3)\n",
    "rawdata1[['From', 'To']] = rawdata1[['From', 'To']].applymap(\n",
    "    lambda x: re.compile('(.*[a-zA-Z]+.*[^\\d]|^)(\\d{1,2})([^\\d].*[a-zA-Z]+.*|$)').sub(expand_year, str(x)))\n",
    "rawdata1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3083db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse date: use dateparser\n",
    "config_dateparser = {'PREFER_DAY_OF_MONTH': 'first'}\n",
    "rawdata1[['From', 'To']] = rawdata1[['From', 'To']].applymap(\n",
    "    lambda d: parse(str(d), settings= config_dateparser))\n",
    "rawdata1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff530fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the dates that could not be parsed correctly\n",
    "print('Observations with dates that could not be parsed correctly')\n",
    "display(rawdata0[rawdata1[['From', 'To']].isnull().values.any(axis= 1)])\n",
    "# Remove the datest that could not be parsed correctly\n",
    "rawdata1.drop(rawdata1[rawdata1[['From', 'To']].isnull().values.any(axis= 1)].index, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b7c29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recalculate Experience column based on parsing\n",
    "rawdata2 = rawdata1.copy()\n",
    "rawdata2['Experience'] = rawdata2['To'] - rawdata2['From']\n",
    "\n",
    "# Check for negative periods (usually are typos) by looking at the original dataset\n",
    "print('Observaions with wrong or missing chronological order')\n",
    "display(rawdata0.loc[rawdata2[rawdata2['Experience'] < timedelta(0)].index])\n",
    "\n",
    "# Remove the observations with wrong cronological order\n",
    "rawdata2.drop(rawdata2[rawdata2['Experience'] < timedelta(0)].index, inplace= True)\n",
    "rawdata2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d70e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d2972d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {rawdata2.Name.unique().shape[0]} different profiles')\n",
    "print(f'There are {rawdata2.Company.unique().shape[0]} different firms')\n",
    "print(f'There are {rawdata2.Role.unique().shape[0]} different roles')\n",
    "print(f'There are {rawdata2.shape[0] - rawdata2.Name.unique().shape[0]} job hops')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12e748d",
   "metadata": {},
   "source": [
    "# Job and Company parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fac7b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata3 = rawdata2.copy()\n",
    "\n",
    "# Translate jobs if necessary\n",
    "googlet = GoogleTranslator(source='auto', target='en')\n",
    "rawdata3[['Role', 'Company']] = rawdata1[['Role', 'Company']].applymap(\n",
    "    lambda x: googlet.translate(x))\n",
    "rawdata3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dec6f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which jobs got translated\n",
    "italian_jobs = rawdata2[(rawdata3[['Role', 'Company']] != rawdata2[['Role', 'Company']]).any(axis= 1)][['Role', 'Company']]\n",
    "italian_jobs = italian_jobs.rename(columns= {'Role': 'Italian Role', 'Company': 'Italian Company'})\n",
    "english_jobs = rawdata3[(rawdata3[['Role', 'Company']] != rawdata2[['Role', 'Company']]).any(axis= 1)][['Role', 'Company']]\n",
    "pd.concat([italian_jobs, english_jobs], axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb5094b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make all lower case\n",
    "print(f'There are NAs: {str(np.where(rawdata3.isna()))}')\n",
    "rawdata3 = rawdata3.dropna()\n",
    "# Remove punctuation\n",
    "rawdata3[['Role', 'Company']] = rawdata3[['Role', 'Company']].applymap(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "# Make lower case\n",
    "rawdata3[['Role', 'Company']] = rawdata3[['Role', 'Company']].applymap(str.lower)\n",
    "# Remove stop-words\n",
    "rawdata3[['Role', 'Company']] = rawdata3[['Role', 'Company']].applymap(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d8eeb3",
   "metadata": {},
   "source": [
    "### In the next session we swap Company and Role columns where necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454eddb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of jobs from https://www.careerbuilder.com/browse\n",
    "jobs_list= []\n",
    "jobs_from_website = pd.read_csv('..\\data\\jobs-from-website.csv', header= None)\n",
    "# Function to extract jobs from strings\n",
    "for i in jobs_from_website.values:\n",
    "    temporary = re.split(r'([a-z])([A-Z])',i[0])\n",
    "    if len(temporary) > 3:\n",
    "        jobs_list += [temporary[0] + temporary[1]]\n",
    "        jobs_list += [temporary[-2] + temporary[-1]]\n",
    "    else:\n",
    "        jobs_list += temporary\n",
    "    if len(temporary) > 4:\n",
    "        for j in range(3,len(temporary)-1,3):\n",
    "            jobs_list += [temporary[j-1] + temporary[j] + temporary[j+1]]\n",
    "# Take only the last word from the job title\n",
    "job_titles = [i.split()[-1] for i in jobs_list]\n",
    "# Some other common job-related words\n",
    "job_titles.extend(['manager', 'director', 'chief', 'president', 'head', 'partner', 'leader', \\\n",
    "    'founder', 'senior', 'member', 'chairman', 'ceo', 'deputy', 'lead'\\\n",
    "    'responsible', 'collaborator', 'crew', 'recruiter', 'planner', 'developer', 'author', \\\n",
    "    'consultant', 'specialist', 'generalist', 'analyst', 'trainer', 'associate', 'officer', 'advisor'\\\n",
    "    'dealer', 'teller', 'coach', 'talent', 'engineer', 'lecturer', 'fellow', 'tutor', 'secretary', \\\n",
    "    'hostess', 'junior', 'assistant', 'trainee', 'student', 'intern', 'graduate', 'scholar'\\\n",
    "    'merchandiser', 'draftsman', 'controller', 'executive', 'designer', 'technician', 'buyer', \\\n",
    "    'researcher', 'worker', 'architect', 'department', 'responsible', 'internship', 'engineer', \\\n",
    "    'scientist', 'operator', 'machinist', 'mechanic', 'installer', 'auditor', 'counsel', 'assessor'])\n",
    "job_list_unique = list(set([i.lower() for i in job_titles]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d176a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Swap Company with Role if necessary\n",
    "rawdata4 = rawdata3.copy()\n",
    "swap_company_role = rawdata3['Company'].apply(lambda x: any([i in job_list_unique for i in x.split()])) & \\\n",
    "    rawdata3['Role'].apply(lambda x: any([i not in job_list_unique for i in x.split()]))\n",
    "rawdata4.loc[swap_company_role, ['Role', 'Company']] = rawdata4.loc[swap_company_role, ['Company', 'Role']].values\n",
    "# Check swaps\n",
    "pd.concat([rawdata3.loc[swap_company_role, ['Company', 'Role']], \n",
    "           rawdata4.loc[swap_company_role, ['Company', 'Role']]], axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a994fab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates based on 'Name', 'Company', 'From' and 'To'\n",
    "rawdata4 = rawdata4[~rawdata4[['Name','Company','From','To']].duplicated(keep= 'first')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2db960",
   "metadata": {},
   "source": [
    "## Normalize Companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c78441f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {rawdata4.Name.unique().shape[0]} different profiles')\n",
    "print(f'There are {rawdata4.Company.unique().shape[0]} different firms')\n",
    "print(f'There are {rawdata4.Role.unique().shape[0]} different roles')\n",
    "print(f'There are {rawdata4.shape[0] - rawdata4.Name.unique().shape[0]} job hops')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217951f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f7e270",
   "metadata": {},
   "source": [
    "### Count vectorization and clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a7cd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "cvfit = cv.fit_transform(rawdata4['Company'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b79c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.printoptions(threshold=np.inf):\n",
    "    for i in list(zip(sorted(cv.vocabulary_),cvfit.sum(axis= 0).tolist()[0])):\n",
    "        if i[1] >= 100 and i[1] < np.inf:\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6083cc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = min(200, rawdata4.shape[0] - 1)\n",
    "kmeans_cv = KMeans(n_clusters=n_clusters, random_state= 892536, n_init= 10)\n",
    "kmeans_cv.fit(cvfit)\n",
    "clusters_cv = kmeans_cv.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a860380a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By manually inspecting the clusters it is possible to see that many do not make sense\n",
    "with np.printoptions(threshold=np.inf):\n",
    "    print(rawdata4['Company'][clusters_cv==0].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533e70c1",
   "metadata": {},
   "source": [
    "### TF-IDF vectorization and clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca51b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer( sublinear_tf= True, stop_words= 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5eb1baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X= vectorizer.fit_transform(rawdata4['Company'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46946b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters= n_clusters, random_state= 892536, n_init= 10)\n",
    "kmeans.fit(X)\n",
    "clusters = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047ad72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By manually inspecting the clusters these make much more sense than before\n",
    "with np.printoptions(threshold=np.inf):\n",
    "    print(rawdata4['Company'][clusters==0].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cbd6cb",
   "metadata": {},
   "source": [
    "### Doc2Vec vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bb48b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddcbe7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_documents = rawdata4['Company'].apply(str.split).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e57d476",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_documents_tagged = [TaggedDocument(d, [i]) for i, d in enumerate(company_documents)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d5f313",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = 20\n",
    "model_d2v = Doc2Vec(company_documents_tagged, min_count= 2, vector_size= vector_size, window=1, workers= 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b380aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_vectors = np.ndarray((0,vector_size))\n",
    "for i in range(len(model_d2v.dv)):\n",
    "    document_vectors = np.vstack((document_vectors,model_d2v.dv.get_vector(i).reshape(1,-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1619c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_d2v = KMeans(n_clusters= n_clusters, random_state= 892536, n_init= 100)\n",
    "kmeans_d2v.fit(document_vectors)\n",
    "clusters_d2v = kmeans_d2v.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50edfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By manually inspecting the clusters these do not make much sense\n",
    "with np.printoptions(threshold=np.inf):\n",
    "    print(rawdata4['Company'][clusters_d2v==0].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7691426",
   "metadata": {},
   "source": [
    "### Google ngram approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056250bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google NGram approach\n",
    "import requests\n",
    "import urllib\n",
    "  \n",
    "def google_ngram_query(query, start_year=2018, \n",
    "             end_year=2019, corpus= 26,\n",
    "             smoothing=2):\n",
    "  \n",
    "    # converting a regular string to \n",
    "    # the standard URL format\n",
    "    # eg: \"geeks for,geeks\" will\n",
    "    # convert to \"geeks%20for%2Cgeeks\"\n",
    "    query = urllib.parse.quote(query)\n",
    "  \n",
    "    # creating the URL\n",
    "    url = 'https://books.google.com/ngrams/json?content=' + query + \\\n",
    "    '&year_start=' + str(start_year) + '&year_end=' + \\\n",
    "    str(end_year) + '&corpus=' + str(corpus) + '&smoothing=' + \\\n",
    "    str(smoothing) + ''\n",
    "  \n",
    "    # requesting data from the above url\n",
    "    response = requests.get(url)\n",
    "    it = 0\n",
    "    while it < 10 and response.status_code != 200:\n",
    "        response = requests.get(url)\n",
    "        it += 1\n",
    "  \n",
    "    # extracting the json data from the response we got\n",
    "    return_data = {}\n",
    "    if response.status_code == 200:\n",
    "        output = response.json()\n",
    "        if len(output) == 0:\n",
    "            # if no data returned from Google\n",
    "            print('No data available for this Ngram.')\n",
    "        else:\n",
    "            # if data returned from Google\n",
    "            for num in range(len(output)):\n",
    "                return_data[output[num]['ngram']] = output[num]['timeseries'][-1]\n",
    "    else:\n",
    "        print('Stale request!')\n",
    "  \n",
    "    return return_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b441aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the HTML\n",
    "t0 = time.time()\n",
    "print(google_ngram_query('house'))\n",
    "print(time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072b1f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm to keep the most unsual word\n",
    "def keep_unusual_word(x):\n",
    "    if not isinstance(x,str):\n",
    "        return(x)\n",
    "    x_out = x.strip().split()\n",
    "    if len(x_out) > 1:\n",
    "        ngram_dict = google_ngram_query(','.join(x_out))\n",
    "        if len(ngram_dict) > 0:\n",
    "            x_out = min(ngram_dict, key= ngram_dict.get)\n",
    "        else:\n",
    "            x_out = ' '.join(x_out)\n",
    "    elif len(x_out) == 1:\n",
    "        x_out = x_out[0]\n",
    "    else:\n",
    "        x_out = np.nan\n",
    "    return(x_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d74593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the code and keep only most unusual word: not that we get some stale requests\n",
    "rawdata4['Company'] = rawdata4['Company'].apply(keep_unusual_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac635c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the results: stale requests meant that some jobs are not parsed correctly\n",
    "rawdata4['Company'].value_counts()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaeeac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By how much the number got reduced\n",
    "len(rawdata3['Company'].unique()), len(rawdata4['Company'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa4e93d",
   "metadata": {},
   "source": [
    "### Manual Company Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c50655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undo changes done before\n",
    "rawdata4 = rawdata3.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e3cb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual company normalization\n",
    "firms = pd.read_csv(r'..\\data\\firms-tiny-sample.csv', keep_default_na= False)\n",
    "firms_names = dict(zip(firms['Company'],firms['Alias']))\n",
    "# Anonymize Italian Motor Valley companies\n",
    "firms_new_names = [row['Alias'] if row['Alias']!='' else row['Company'] for irow, row in firms.iterrows()]\n",
    "firms_number = dict(zip(firms_new_names,firms['Number of Employees']))\n",
    "firms_sector = dict(zip(firms_new_names,firms['Type']))\n",
    "firms_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b143f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to look for nicknames and replace\n",
    "def company_normalization(x):\n",
    "    out = None\n",
    "    for i in firms_names.keys():\n",
    "        if re.search(i,x):\n",
    "            if firms_names[i]:\n",
    "                out = firms_names[i]\n",
    "            else:\n",
    "                out = i\n",
    "    return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7a227a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata5 = rawdata4.copy()\n",
    "rawdata5['Company'] = rawdata5['Company'].apply(lambda x: company_normalization(x))\n",
    "rawdata5.dropna(inplace= True)\n",
    "rawdata5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d42f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {rawdata5.Name.unique().shape[0]} different profiles')\n",
    "print(f'There are {rawdata5.Company.unique().shape[0]} different firms')\n",
    "print(f'There are {rawdata5.Role.unique().shape[0]} different roles')\n",
    "print(f'There are {rawdata5.shape[0] - rawdata4.Name.unique().shape[0]} job hops')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7629067e",
   "metadata": {},
   "source": [
    "## Role Normalization\n",
    "\n",
    "This section was tested but not used nor reported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b10862e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "role_documents = rawdata4['Role'].apply(str.split).tolist()\n",
    "role_documents_tagged = [TaggedDocument(d, [i]) for i, d in enumerate(role_documents)]\n",
    "role_d2v = Doc2Vec(role_documents_tagged, min_count= 2, vector_size= vector_size, window=1, workers= 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612e36ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "role_vectors = np.ndarray((0,vector_size))\n",
    "for i in range(len(model_d2v.dv)):\n",
    "    role_vectors = np.vstack((role_vectors,role_d2v.dv.get_vector(i).reshape(1,-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d837ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "kmeans_role_d2v = KMeans(n_clusters= 200, random_state= 892536, n_init= 100)\n",
    "kmeans_role_d2v.fit(role_vectors)\n",
    "clusters_role_d2v = kmeans_role_d2v.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a720c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "with np.printoptions(threshold=np.inf):\n",
    "    print(rawdata4['Role'][clusters_role_d2v==0].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51cd6a9",
   "metadata": {},
   "source": [
    "# Prepare Job Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9873e42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove people with only one experience\n",
    "rawdata5 = rawdata5[rawdata5['Name'].duplicated(keep= False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebceb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {rawdata5.Name.unique().shape[0]} different profiles')\n",
    "print(f'There are {rawdata5.Company.unique().shape[0]} different firms')\n",
    "print(f'There are {rawdata5.Role.unique().shape[0]} different roles')\n",
    "print(f'There are {rawdata5.shape[0] - rawdata5.Name.unique().shape[0]} job hops')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e599976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm to extract job hops\n",
    "job_hops = []\n",
    "job_hops_ext = []\n",
    "role_hops = []\n",
    "role_hops_ext = []\n",
    "for name in rawdata5['Name'].unique():\n",
    "    df = rawdata5[rawdata5['Name']==name].sort_values(by= 'From')\n",
    "    row_queue = []\n",
    "    for i, (irow, row) in enumerate(df.iterrows()):\n",
    "        if i != 0:\n",
    "            for j in range(len(row_queue)):\n",
    "                if row['From'] >= row_queue[j]['To']:\n",
    "                    job_hops.append((row_queue[j]['Company'], row['Company']))\n",
    "                    role_hops.append((row_queue[j]['Role'], row['Role']))\n",
    "                    if row_queue[j]['Company'] != row['Company']:\n",
    "                        job_hops_ext.append((row_queue[j]['Company'], row['Company']))\n",
    "                    if row_queue[j]['Role'] != row['Role']:\n",
    "                        role_hops_ext.append((row_queue[j]['Role'], row['Role']))\n",
    "                    row_queue.pop(j)\n",
    "                    break      \n",
    "        row_queue.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad1215e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(job_hops), len(job_hops_ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37370811",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(role_hops), len(role_hops_ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e718954e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique job hops\n",
    "job_hop_ext_unique = list(set(job_hops_ext))\n",
    "job_hop_ext_unique_wt = [job_hops_ext.count(i) for i in job_hop_ext_unique]\n",
    "job_hop_ext_unique_weighted = [i1 + (dict(weight = i2/firms_number[i1[0]], weight_tgt = i2/firms_number[i1[1]]),) \\\n",
    "                               for i1, i2 in zip(job_hop_ext_unique, job_hop_ext_unique_wt)]\n",
    "print(f'There are {len(job_hops_ext)} job hops of which {len(job_hop_ext_unique)} are unique')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24a5097",
   "metadata": {},
   "source": [
    "`job_hop_ext_unique`, `job_hop_ext_unique_wt` and `job_hop_ext_unique_weighted` define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a251df64",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/job-hop-net.dat','w') as file:\n",
    "    for job_link, weight in zip(job_hop_ext_unique, job_hop_ext_unique_wt):\n",
    "        file.write('\\t'.join((\n",
    "            job_link[0],\n",
    "            firms_sector[job_link[0]],\n",
    "            job_link[1],\n",
    "            firms_sector[job_link[1]],\n",
    "            str(weight),\n",
    "            str(weight/firms_number[job_link[0]]),\n",
    "            str(weight/firms_number[job_link[1]]))\n",
    "        ) + '\\n')\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a6e58a",
   "metadata": {},
   "source": [
    "### End of file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1216196",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "job-hop-net-env",
   "language": "python",
   "name": "job-hop-net-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
